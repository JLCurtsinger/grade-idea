name: Ping Search Engines

on:
  push:
    branches: [ "main" ]

jobs:
  ping:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install deps
        run: npm ci

      # Wait up to ~5 minutes for Vercel's auto-deploy to go live
      - name: Wait for site to be live
        run: |
          URL="https://www.gradeidea.cc/sitemap.xml"
          for i in {1..60}; do
            code=$(curl -s -o /dev/null -w "%{http_code}" "$URL")
            echo "Attempt $i: $code"
            if [ "$code" = "200" ]; then
              exit 0
            fi
            sleep 5
          done
          echo "Site not live yet (sitemap still not 200)."
          exit 1

      - name: Validate sitemap like Googlebot
        run: |
          URL="https://www.gradeidea.cc/sitemap.xml"
          echo "HEAD headers:" && curl -sI "$URL"
          echo "Fetch as Googlebot:" && curl -s -A "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" -o /tmp/sm.xml -w "\nHTTP:%{http_code}\n" "$URL"
          file -bi /tmp/sm.xml
          head -n 5 /tmp/sm.xml
          ct=$(file -bi /tmp/sm.xml)
          echo "Content-Type: $ct"
          [[ "$ct" =~ xml ]] || { echo "Not XML ($ct)"; exit 1; }

      - name: Quick XML well-formedness check
        run: |
          sudo apt-get update -y && sudo apt-get install -y libxml2-utils
          xmllint --noout /tmp/sm.xml

      - name: Fetch robots.txt as Googlebot
        run: |
          curl -s -A "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" -o /tmp/robots-gb.txt -w "\nHTTP:%{http_code}\n" https://www.gradeidea.cc/robots.txt
          head -n 5 /tmp/robots-gb.txt

      - name: Sample sitemap URLs resolve 200
        run: |
          grep -Eo '<loc>[^<]+' /tmp/sm.xml | sed 's/<loc>//' | head -n 25 | while read url; do
            code=$(curl -s -o /dev/null -w "%{http_code}" "$url")
            echo "$code $url"
            if [ "$code" -ge 400 ]; then echo "Bad URL in sitemap: $url"; exit 1; fi
          done

      - name: Assert robots.txt exposes sitemap
        run: |
          curl -s https://www.gradeidea.cc/robots.txt | tee /tmp/robots.txt
          grep -qi "^sitemap: https://www.gradeidea.cc/sitemap.xml" /tmp/robots.txt || { echo "robots.txt missing Sitemap line"; exit 1; }

      - name: Notify search engines that still listen (Bing/IndexNow)
        run: npm run ping-search
        env:
          INDEXNOW_KEY: ${{ secrets.INDEXNOW_KEY }}